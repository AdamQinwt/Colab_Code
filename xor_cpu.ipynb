{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamQinwt/Colab_Code/blob/master/xor_cpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeOvIWgqdVNN",
        "colab_type": "text"
      },
      "source": [
        "# Estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5rfKwbedR8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!git clone https://github.com/tensorflow/models\n",
        "#!python models/samples/core/get_started/premade_estimator.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "problem='xor'\n",
        "BATCH_SIZE=4\n",
        "LEARNING_RATE=0.1\n",
        "IMG_SIZE=2\n",
        "DATASET_NAME='/content/drive/My Drive/datasets/'+problem+'.tfrecords'\n",
        "ROOT='//content/drive/My Drive/'+problem+'/'\n",
        "CLASS_NUM=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEEJ2NZRK426",
        "colab_type": "text"
      },
      "source": [
        "## input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HshrfnQJK9B7",
        "colab_type": "text"
      },
      "source": [
        "### train input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Q1mX8jK4Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parser_data(serialized_example):\n",
        "    context, sequence = tf.parse_single_sequence_example(serialized_example,\n",
        "                      context_features={\n",
        "                          'features': tf.FixedLenFeature([IMG_SIZE],\n",
        "                          tf.float32),\n",
        "                          'label': tf.FixedLenFeature(\n",
        "                              [1], tf.float32)\n",
        "                      })\n",
        "    feat = context['features']\n",
        "#    feat=tf.reshape(feat,IMG_SHAPE)\n",
        "    label = context['label']\n",
        "    label = tf.cast(tf.reshape(label, [1]),tf.int32)\n",
        "    return feat, label\n",
        "\n",
        "def train_input_fn(file_name, batch_size):\n",
        "  \"\"\"An input function for training\"\"\"\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.TFRecordDataset([file_name])\n",
        "  # dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "  # Shuffle, repeat, and batch the examples.\n",
        "  dataset = dataset.map(parser_data).shuffle(2).repeat().batch(batch_size)\n",
        "\n",
        "  # Return the read end of the pipeline.\n",
        "  return dataset.make_one_shot_iterator().get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsoVDa9SLBUd",
        "colab_type": "text"
      },
      "source": [
        "### test(eval) input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw0G6NHgO-Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_input_fn(file_name, batch_size):\n",
        "  \"\"\"An input function for training\"\"\"\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.TFRecordDataset([file_name])\n",
        "  # dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "  # Shuffle, repeat, and batch the examples.\n",
        "  dataset = dataset.map(parser_data).shuffle(2).batch(batch_size)\n",
        "\n",
        "  # Return the read end of the pipeline.\n",
        "  return dataset.make_one_shot_iterator().get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKmpmbNDIxMh",
        "colab_type": "text"
      },
      "source": [
        "## model_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQfTJKpvJX7h",
        "colab_type": "text"
      },
      "source": [
        "### create_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nqdSdHxJWKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  l = tf.keras.layers\n",
        "  return tf.keras.Sequential(\n",
        "      [\n",
        "          l.Dense(3, activation=tf.nn.sigmoid),\n",
        "          l.Dense(CLASS_NUM)\n",
        "      ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTLSxy5hJoZ6",
        "colab_type": "text"
      },
      "source": [
        "### define the estimator functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj5pq4NiIwO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  \"\"\"The model_fn argument for creating an Estimator.\"\"\"\n",
        "  model = create_model()\n",
        "  image = features\n",
        "  if isinstance(image, dict):\n",
        "    image = features['image']\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    logits = model(image, training=False)\n",
        "    predictions = {\n",
        "        'classes': tf.argmax(logits, axis=1),\n",
        "        'probabilities': tf.nn.softmax(logits),\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=tf.estimator.ModeKeys.PREDICT,\n",
        "        predictions=predictions,\n",
        "        export_outputs={\n",
        "            'classify': tf.estimator.export.PredictOutput(predictions)\n",
        "        })\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
        "\n",
        "    logits = model(image, training=True)\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "    accuracy = tf.metrics.accuracy(\n",
        "        labels=labels, predictions=tf.argmax(logits, axis=1))\n",
        "    # Name tensors to be logged with LoggingTensorHook.\n",
        "    tf.identity(LEARNING_RATE, 'learning_rate')\n",
        "    tf.identity(loss, 'cross_entropy')\n",
        "    tf.identity(accuracy[1], name='train_accuracy')\n",
        "    # Save accuracy scalar to Tensorboard output.\n",
        "    tf.summary.scalar('train_accuracy', accuracy[1])\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=tf.estimator.ModeKeys.TRAIN,\n",
        "        loss=loss,\n",
        "        train_op=optimizer.minimize(loss, tf.train.get_or_create_global_step()))\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    logits = model(image, training=False)\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=tf.estimator.ModeKeys.EVAL,\n",
        "        loss=loss,\n",
        "        eval_metric_ops={\n",
        "            'accuracy':\n",
        "                tf.metrics.accuracy(\n",
        "                    labels=labels, predictions=tf.argmax(logits, axis=1)),\n",
        "        })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75COMo76PuKg",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TN7SlJyP2Jx",
        "colab_type": "code",
        "outputId": "24ed29de-9127-4e0a-8a8e-4ba40ecf8650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "classifier = tf.estimator.Estimator(\n",
        "        model_fn=model_fn,\n",
        "        params={\n",
        "            'data_format':'channels_last'\n",
        "        },\n",
        "        model_dir=ROOT)\n",
        "# for i in range(10):\n",
        "t=classifier.train(\n",
        "    input_fn=lambda:train_input_fn(DATASET_NAME,BATCH_SIZE),\n",
        "    steps=2000)\n",
        "print('train')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b8d1ba1fa646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     steps=2000)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Estimator' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2z-aHMnjmsF",
        "colab_type": "code",
        "outputId": "a9578c77-aaa6-43e2-80dc-6205fd7a79fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Evaluate the model.\n",
        "eval_result = classifier.evaluate(\n",
        "    input_fn=lambda:test_input_fn(DATASET_NAME,BATCH_SIZE)\n",
        "    )\n",
        "print(eval_result)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'accuracy': 1.0, 'loss': 1.9430798e-05, 'global_step': 4000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RxO0hnTGBYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "2f212e09-982d-475e-81a4-a7137b541d74"
      },
      "source": [
        "predictions = classifier.predict(\n",
        "    input_fn=lambda:test_input_fn(DATASET_NAME,\n",
        "    batch_size=1))\n",
        "for p in predictions:\n",
        "  print(p)\n",
        "#print(predictions)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'classes': 1, 'probabilities': array([1.3469916e-05, 9.9998653e-01], dtype=float32)}\n",
            "{'classes': 0, 'probabilities': array([9.9995530e-01, 4.4651082e-05], dtype=float32)}\n",
            "{'classes': 1, 'probabilities': array([1.8630824e-05, 9.9998140e-01], dtype=float32)}\n",
            "{'classes': 0, 'probabilities': array([9.9999905e-01, 9.9239662e-07], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ond0dk2_azw-",
        "colab_type": "code",
        "outputId": "2419e98f-fa86-4a6e-9460-6777040253b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXmgGAKti9u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clear\n",
        "# !rm -r /tmp/tmp*"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}